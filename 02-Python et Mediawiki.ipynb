{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introduction</a></span></li><li><span><a href=\"#Création-du-compte-de-bot\" data-toc-modified-id=\"Création-du-compte-de-bot-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Création du compte de bot</a></span></li><li><span><a href=\"#Utilisation-de-pywikiapi\" data-toc-modified-id=\"Utilisation-de-pywikiapi-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Utilisation de pywikiapi</a></span><ul class=\"toc-item\"><li><span><a href=\"#Login\" data-toc-modified-id=\"Login-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Login</a></span></li><li><span><a href=\"#Lire-le-contenu-depuis-l'API\" data-toc-modified-id=\"Lire-le-contenu-depuis-l'API-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Lire le contenu depuis l'API</a></span></li><li><span><a href=\"#Éditer-wikipast-en-python\" data-toc-modified-id=\"Éditer-wikipast-en-python-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Éditer wikipast en python</a></span></li><li><span><a href=\"#Conclusion\" data-toc-modified-id=\"Conclusion-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Conclusion</a></span></li></ul></li><li><span><a href=\"#Quelques-ressources-utiles-pour-Wikipast\" data-toc-modified-id=\"Quelques-ressources-utiles-pour-Wikipast-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Quelques ressources utiles pour Wikipast</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Comme nous le verrons plus tard dans le cours, Wikipedia est peuplé par de nombreux bots qui sont les utilisateurs les plus prolifiques en terme de modifications brutes. Wikimedia, le logiciel qui fait tourner Wikipedia, possède donc une API très complète permettant de réaliser facilement la plupart des tâches basiques, à savoir retrouver des pages et les modifier.\n",
    "\n",
    "La documentation de cette API est trouvable en ligne à cette adresse: https://www.mediawiki.org/wiki/API:Main_page/fr. Cette API est utilisable à l'aide de requêtes web classiques, toutefois il existe plusieurs \"wrappers\" python permettant de l'utiliser plus facilement dans ce langage. Nous utiliserons dans ce tutoriel le wrapper [`pywikiapi`](https://github.com/nyurik/pywikiapi) qui est très léger et certainement suffisant pour notre cours, toutefois vous pouvez également jeter un coup d'oeil aux autres wrappers: https://www.mediawiki.org/wiki/API:Client_code.\n",
    "\n",
    "\n",
    "# Création du compte de bot\n",
    "Pour faire des appels de lectures à l'API, il n'est pas nécessaire d'avoir de compte, toutefois pour pouvoir modifier les pages, il est nécessaire de créer un compte de bot. Les étapes de créations sont les suivantes:\n",
    "\n",
    "1. se rendre sur la page http://wikipast.epfl.ch/wikipast/index.php/Sp%C3%A9cial:BotPasswords/,\n",
    "2. Créer un nouveau robot en choisissant les bons droits de modifications en fonction de vos besoins (par exemple: \"Modification de gros volumes\", \"Modifier des pages existantes\", \"Créer, modifier et déplacer des pages\", \"Importer de nouveaux fichiers\", \"Téléverser, remplacer et renommer des fichiers\")\n",
    "3. La page suivante donne le mot de passe du bot, il faut bien le sauvegarder car il ne sera plus affiché, de plus il faut le garder secrer car il permet de modifier des pages sur Wikipast.\n",
    "\n",
    "\n",
    "Vous pouvez maintenant rentrer les identifiants du bots dans la cellule ci-dessous pour pouvoir l'utiliser dans ce tutoriel (et ne pas oublier de l'exécuter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = 'Username@yourbot'\n",
    "password = 'xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilisation de pywikiapi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pywikiapi import Site"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pywikiapi` utilise un objet `Site` pour faire ses requête. Cet objet prend l'adresse de l'API et permet d'abstraire la plupart des appels à l'API derrière une syntaxe python simple. Seules deux fonctionalités de l'API sont implémentées spécifiquement dans `pywikiapi`: `login` et `query`, toutefois, tous les autres fonctionnalités de l'API peuvent être également invoquées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site = Site('http://wikipast.epfl.ch/wikipast/api.php') # Définition de l'adresse de l'API\n",
    "site.no_ssl = True # Désactivation du https, car pas activé sur wikipast\n",
    "site.login(user, password) # Login du bot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons maintenant appelé la fonction [`login`](http://wikipast.epfl.ch/wikipast/api.php?action=help&modules=login) de l'API.\n",
    "\n",
    "## Lire le contenu depuis l'API\n",
    "Les deux autres fonctions implémentées qui appellent explicitement l'API sont `site.query` et `site.query_pages`. En voici un exemple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "# Cherche dans toutes les pages celles commençant par Leopold\n",
    "for res in site.query(list='allpages', apprefix='Mahatma'): \n",
    "    results.append(res)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons extraire de cette liste de résultats le nom des pages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_names = []\n",
    "for res in results[0]['allpages']: # aller dans le dictionnaire nesté\n",
    "    pages_names.append(res['title'])\n",
    "pages_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une fois ces noms extraits, nous pouvons utiliser `site.query_pages` pour extraire certaines informations sur ces pages. La cellule suivante trouves les pages ayant un titre dans page_names et retourne les attributs \"categories\", \"links\", \"extlinks\" comme décrits dans la documentation: http://wikipast.epfl.ch/wikipast/api.php?action=help&modules=query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_pages = []\n",
    "\n",
    "for res in site.query_pages(titles=pages_names, prop=['categories', 'links', 'extlinks']): \n",
    "    results_pages.append(res)\n",
    "results_pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Toutefois, ce qui va souvent nous intéresser est le contenu textuelle d'une page du wiki. `pywikiapi` n'implémente pas directement des fonctions pour faire cela, mais nous pouvons tout de même l'utiliser grâce à un appel direct à `Site`.\n",
    "\n",
    "En effet, en écrivant `site('parse', page='Mahatma Gandhi', prop=['wikitext'])`, `pywikiapi` fait directement un appel à la fonction `parse` de l'API, nous pouvons ensuite passer les autres arguments directement dans l'appel de la fonction et ils seront transmis à l'API. Pour plus de détails sur les arguments possibles, il faut de nouveau se référer à la documentation: http://wikipast.epfl.ch/wikipast/api.php?action=help&modules=parse. Dans celle-ci, nous pouvons voir que `parse` prend un argument `page` qui indique la page à parser et un argument `prop` qui peut entre autre prendre la valeur `sections` qui fait retourner à `parse` une liste de ses sections. En voici le résultat:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res = site('parse', page='Mahatma Gandhi', prop=['sections'])\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant que nous avons les indexs des sections, nous pouvons récupérer le wikitext d'une section particulière:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = site('parse', page='Mahatma Gandhi', prop=['wikitext'], section=None)\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons maintenant écrire une fonction qui prend un nom de page et nous retourne son wikitext et une autre qui nous retourne ses sections:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wiki_text(page, section=None):\n",
    "    result = site('parse', page=page, prop=['wikitext'], section=section)\n",
    "    return result['parse']['wikitext']\n",
    "\n",
    "def get_sections(page):\n",
    "    result = site('parse', page=page, prop=['sections'])\n",
    "    return result['parse']['sections']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous pouvez maintenant tester les fonctions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_sections('Mahatma Gandhi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_wiki_text('Mahatma Gandhi', section=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voici un exemple de comment parser les entrées de datafication depuis le wikitext:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sections = get_sections('Mahatma Gandhi')\n",
    "section_biographie_id = None\n",
    "for section in sections:\n",
    "    if section['line'] == 'Biographie':\n",
    "        section_biographie_id = section['index']\n",
    "wikitext = get_wiki_text(\"Mahatma Gandhi\", section=section_biographie_id)\n",
    "entries = [entry for entry in wikitext.split(\"\\n\") if entry.startswith('*')]\n",
    "entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Éditer wikipast en python\n",
    "\n",
    "Il nous reste donc à montrer comment éditer une page, l'API est défini sur cette page: http://wikipast.epfl.ch/wikipast/api.php?action=help&modules=edit.\n",
    "\n",
    "Vous pouvez librement vous créer une page bac à sable où utiliser la page [bacasable](http://wikipast.epfl.ch/wikipast/index.php/Bacasable).\n",
    "\n",
    "Nous allons donc créer une nouvelle page et l'éditer depuis python, vous pouvez à chaque étape voir le résultat de vos modifications sur la page correspondante de wikipast.\n",
    "\n",
    "Commençons par créer la nouvelle page (modifiez la variable `titre`), soyez prudents, ce bout de code écrase entièrement la page (même s'il existait déjà):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titre = 'Bacasable - tutoriel'\n",
    "\n",
    "site('edit', title=titre,\n",
    "     text='Ceci est une nouvelle page.\\nAvec peu de contenu.',\n",
    "     token=site.token())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est important de noter que pour modifier une page (création, modification, suppression), il faut absolument être connecté et avoir un token d'édition. Ce dernier peut être obtenu en appelant `site.token()` et devra donc être ajouté comme argument à chaque fonction éditant une page.\n",
    "\n",
    "Rajoutons maintenant deux nouvelles sections à notre page:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site('edit', title=titre,\n",
    "     section='new',\n",
    "     sectiontitle='Nouvelle section 1 de test',\n",
    "     text='Ceci est le texte de ma nouvelle section.',\n",
    "     token=site.token())\n",
    "site('edit', title=titre,\n",
    "     section='new',\n",
    "     sectiontitle='Nouvelle section 2 de test',\n",
    "     text='Ceci est le texte de ma nouvelle section.',\n",
    "     token=site.token())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons maintenant ajouter du texte à une de nos sections grâce aux arguments `prependtext` et `appendtext`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site('edit', title=titre,\n",
    "     section=2,\n",
    "     prependtext='Rajoutons du texte avant.\\n', # notez l'ajout d'un retour à la fin du texte ajouté\n",
    "     token=site.token())\n",
    "\n",
    "site('edit', title=titre,\n",
    "     section=2,\n",
    "     appendtext='\\nRajoutons du texte et après.\\n',\n",
    "     token=site.token())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ou écraser le texte, mais d'une seule section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site('edit', title=titre,\n",
    "     section=2,\n",
    "     text='Nouveau texte', # notez l'ajout d'un retour à la fin du texte ajouté\n",
    "     token=site.token())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le seul moyen d'être plus fin dans la modification de la page est de récupérer le wikitext de la page, de modifier sa string, puis d'écraser la page avec le texe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = get_wiki_text(titre)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text.replace('Nouvelle section', 'Sous-titre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site('edit', title=titre,\n",
    "    text=text,\n",
    "    token=site.token())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Nous avons donc vu comment récupérer des informations des pages de wikipast et comment éditer les pages. Toutefois, beaucoup d'options de l'API n'ont pas été couvertes ici, il est donc judicieux de s'y référer lorsque vous créerez vos bots afin de ne pas manquer une fonctionnalité qui pourrait déjà être implémentée avant de la réinventer vous-même."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quelques ressources utiles pour Wikipast\n",
    "\n",
    "Afin de pouvoir traiter les ~670'000 pages présentes sur Wikipast, quelques optimisations sont nécessaires.\n",
    "\n",
    "Voici donc quelques bouts de codes permettant de parcourir toutes les pages efficacement et deux fichiers json pré-calculés qui vous permettront de ne pas avoir à la refaire trop souvent. \n",
    "\n",
    "Les fichiers `json.gz` sont disponibles sur le github du tutoriel dans l'onglet [releases](https://github.com/dhlab-epfl/HUM-365-tutorials/releases).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm # Libraire utile pour voir le progrès d'une boucle\n",
    "import multiprocessing as mp # Librairie d'execution parallèle\n",
    "import gzip # Permet d'économiser beaucoup de place en compressant les fichiers json\n",
    "import json\n",
    "\n",
    "# Read a gzipped json file\n",
    "def load_gzip_json(path):\n",
    "    with gzip.GzipFile(path, 'r') as infile:\n",
    "        return json.loads(infile.read().decode('utf-8'))\n",
    "\n",
    "def write_gzip_json(data, path):\n",
    "    with gzip.GzipFile(path, 'w') as outfile:\n",
    "        outfile.write(json.dumps(data, ensure_ascii=False).encode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce premier bout de code permet de récupérer les titres et id de toutes les pages du wiki, il prend ~1h à être exécuté. La version actuelle a été compilée le 15.03."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_pages = []\n",
    "# for r in tqdm(site.query(list='allpages')):\n",
    "#     for page in r['allpages']:\n",
    "#         all_pages.append(page)\n",
    "# write_gzip_json(all_pages, 'all_pages.json.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pages = load_gzip_json('./all_pages.json.gz')\n",
    "all_pages[100010:100020]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce deuxième bout de code récupère la plupart des infos intéressante que l'on pourrait vouloir d'une page. Il lance le processus en parralèle sur 16 threads, rendant le tout beaucoup plus rapide. Le pattern de code de parallèlisation peut-être réutiliser pour vos propre bots qui devraient traiter beaucoup de pages.\n",
    "\n",
    "Attention, ce code qui traite autant de page est lourd pour nos serveurs et est long à éxecuter (~2h30). La version actuelle a été compilée le 15.03."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_all_page_infos(page):\n",
    "#     return site('parse', page=page, prop=['wikitext','links',\n",
    "#                                           'externallinks', 'categories',\n",
    "#                                           'revid', 'images', 'sections'])\n",
    "\n",
    "# pages_titles = [page['title'] for page in all_pages] # la syntaxe est une list comprehension, c.f. https://docs.python.org/fr/3/tutorial/datastructures.html#list-comprehensions\n",
    "\n",
    "# # Création d'une pool avec 16 process\n",
    "# pool = mp.Pool(16)\n",
    "# # Création d'une barre de chargement et d'une fonction pour la mettre à jour.\n",
    "# pbar = tqdm(total=len(pages_titles))\n",
    "# def update(*a):\n",
    "#     pbar.update()\n",
    "\n",
    "# # Récupération de tous les résultats en parallèle\n",
    "# results = []\n",
    "# for page in pages_titles:\n",
    "#     results.append(pool.apply_async(get_all_page_infos, args=(page,), callback=update))\n",
    "# pool.close()\n",
    "# pool.join()\n",
    "\n",
    "# # Certaines requêtes ont inévitablement raté, il faut donc récupérer le reste\n",
    "# all_results = []\n",
    "# num_failures = 0\n",
    "# for idx, result in enumerate(results):\n",
    "#     try:\n",
    "#         all_results.append(result.get()['parse'])\n",
    "#     except:\n",
    "#         num_failures += 1\n",
    "# print(f\"There is {num_failures} that failed.\")\n",
    "\n",
    "# # Trouvons maintenant les titre des pages qui ont été récupérées\n",
    "# valid_pages_titles = set([result['title'] for result in all_results])\n",
    "    \n",
    "# # Trouvons maintenant les pages qui n'ont pas été récupérées\n",
    "# missing_pages_titles = set(pages_titles).difference(valid_pages_titles)\n",
    "\n",
    "# # Et récuperons les\n",
    "# for page in missing_pages_titles:\n",
    "#     all_results.append(get_all_page_infos(page))\n",
    "    \n",
    "# print(f\"We now have {len(all_results)} out of {len(all_pages)}.\")\n",
    "\n",
    "\n",
    "# # Finalement, écrivons le fichier avec les résultats\n",
    "# write_gzip_json(all_results, 'all_data.json.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = load_gzip_json('./all_data.json.gz')\n",
    "all_data[100000:100002]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ces deux fichiers json peuvent donc être utiles pour explorer les données sans avoir à faire des querys sur toute la base de données."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalement, écrivons deux query qui peuvent être utiles comme points de départ pour un bot. \n",
    "\n",
    "La première nous permet de récuper tous les liens présents sur la page de biographies et qui peut être utile comme point de départ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "site('parse', page='Biographies', prop='links')['parse']['links']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La deuxième utilise le fait que certains bots ont ajouté un lien externe vers l'attribut [Q5](https://www.wikidata.org/wiki/Q5) de wikidata qui indique que la page parle d'un humain, cela peut donc être utilisé pour récupérer toutes les pages parlants d'humains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérifie si https://www.wikidata.org/wiki/Q5 est dans la liste de liens externes\n",
    "# Utilise une liste comprehension avec un filtre conditionnel\n",
    "humans = [data['title'] for data in all_data if \n",
    "          'externallinks' in data and\n",
    "          'https://www.wikidata.org/wiki/Q5' in data['externallinks']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"There is {len(humans)} pages about humans.\")\n",
    "humans[10000:10010]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une dernière note, si vous faites des modifications massives. Soyez prudents en vérifiant avant chaque modification que le `revid` de la page que vous avez récuperé avant modification est le même que celui de la page qui va être modifiée. C'est-à-dire, si vous précalculez toutes les pages à être modifiées, récuperez également leurs `revid`. Ensuite, au moment de la modification, vérifier que le `revid` de la page qui est en ligne est le même que le vôtre, sinon vous allez écrasé une nouvelle modification."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
